{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Architecture \u00b6 Elemental is a toolkit to build an immutable Linux distribution. It's primary purpose is to run Rancher and its corresponding Kubernetes distributions RKE2 and k3s . But it can be configured for any other workload. That said, the following documentation focusses on a Rancher use-case. Initial node configurations is done using a cloud-init style approach and all further maintenance is done using Kubernetes operators. Use Cases \u00b6 The OS built by Elemental is intended to be run as the operating system beneath a Rancher Multi-Cluster Management server or as a node in a Kubernetes cluster managed by Rancher. As such it also allows you to build stand alone Kubernetes clusters that run an embedded and smaller version of Rancher to manage the local cluster. A key attribute of Elemental is that it is managed by Rancher and thus Rancher will exist either locally in the cluster or centrally with Rancher Multi-Cluster Manager. OCI Image based \u00b6 Elemental is an image based distribution with an A/B style update mechanism. One first runs on a read-only image A and to do an upgrade pulls a new read only image B and then reboots the system to run on B. What is unique about Elemental is that the runtime images come from OCI Images. Not an OCI Image containing special artifacts, but an actual Docker runnable image that is built using standard Docker build processes. Elemental is built using normal docker build and if you wish to customize the OS image all you need to do is create a new Dockerfile . rancherd \u00b6 Elemental includes no container runtime, Kubernetes distribution, or Rancher itself. All of these assests are dynamically pulled at runtime. All that is included in Elemental is rancher system agent which is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update to containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot. cloud-init \u00b6 Elemental is initially configured using a simple version of cloud-init . It is not expected that one will need to do a lot of customization to Elemental as the core OS's sole purpose is to run Rancher and Kubernetes and not serve as a generic Linux distribution. Elemental Operator \u00b6 Elemental includes an operator that is responsible for managing OS upgrades and managing a secure device inventory to assist with zero touch provisioning. See the full operator docs at Elemental-operator Elemental Teal \u00b6 Elemental Teal is based off of SUSE Linux Enterprise (SLE) Micro for Rancher. There is no specific dependency on SLE beyond that Elemental assumes the underlying distribution is based on systemd. We choose SLE Micro for Rancher for obvious reasons, but beyond that Elemental Teal provides a stable layer to build upon that is well tested and has paths to commercial support, if one chooses.","title":"Architecture"},{"location":"#architecture","text":"Elemental is a toolkit to build an immutable Linux distribution. It's primary purpose is to run Rancher and its corresponding Kubernetes distributions RKE2 and k3s . But it can be configured for any other workload. That said, the following documentation focusses on a Rancher use-case. Initial node configurations is done using a cloud-init style approach and all further maintenance is done using Kubernetes operators.","title":"Architecture"},{"location":"#use-cases","text":"The OS built by Elemental is intended to be run as the operating system beneath a Rancher Multi-Cluster Management server or as a node in a Kubernetes cluster managed by Rancher. As such it also allows you to build stand alone Kubernetes clusters that run an embedded and smaller version of Rancher to manage the local cluster. A key attribute of Elemental is that it is managed by Rancher and thus Rancher will exist either locally in the cluster or centrally with Rancher Multi-Cluster Manager.","title":"Use Cases"},{"location":"#oci-image-based","text":"Elemental is an image based distribution with an A/B style update mechanism. One first runs on a read-only image A and to do an upgrade pulls a new read only image B and then reboots the system to run on B. What is unique about Elemental is that the runtime images come from OCI Images. Not an OCI Image containing special artifacts, but an actual Docker runnable image that is built using standard Docker build processes. Elemental is built using normal docker build and if you wish to customize the OS image all you need to do is create a new Dockerfile .","title":"OCI Image based"},{"location":"#rancherd","text":"Elemental includes no container runtime, Kubernetes distribution, or Rancher itself. All of these assests are dynamically pulled at runtime. All that is included in Elemental is rancher system agent which is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update to containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot.","title":"rancherd"},{"location":"#cloud-init","text":"Elemental is initially configured using a simple version of cloud-init . It is not expected that one will need to do a lot of customization to Elemental as the core OS's sole purpose is to run Rancher and Kubernetes and not serve as a generic Linux distribution.","title":"cloud-init"},{"location":"#elemental-operator","text":"Elemental includes an operator that is responsible for managing OS upgrades and managing a secure device inventory to assist with zero touch provisioning. See the full operator docs at Elemental-operator","title":"Elemental Operator"},{"location":"#elemental-teal","text":"Elemental Teal is based off of SUSE Linux Enterprise (SLE) Micro for Rancher. There is no specific dependency on SLE beyond that Elemental assumes the underlying distribution is based on systemd. We choose SLE Micro for Rancher for obvious reasons, but beyond that Elemental Teal provides a stable layer to build upon that is well tested and has paths to commercial support, if one chooses.","title":"Elemental Teal"},{"location":"ami/","text":"Amazon AMIs \u00b6 AMIs for RancherOS are published under the owner ID 275947076441 in the us-west-1 and us-west-2 regions currently. aws --region = us-west-1 ec2 describe-images --owners 275947076441 aws --region = us-west-2 ec2 describe-images --owners 275947076441","title":"Amazon AMIs"},{"location":"ami/#amazon-amis","text":"AMIs for RancherOS are published under the owner ID 275947076441 in the us-west-1 and us-west-2 regions currently. aws --region = us-west-1 ec2 describe-images --owners 275947076441 aws --region = us-west-2 ec2 describe-images --owners 275947076441","title":"Amazon AMIs"},{"location":"architecture/","text":"Architecture \u00b6 Elemental is a toolkit to build an immutable Linux distribution. It's primary purpose is to run Rancher and its corresponding Kubernetes distributions RKE2 and k3s . But it can be configured for any other workload. That said, the following documentation focusses on a Rancher use-case. Initial node configurations is done using a cloud-init style approach and all further maintenance is done using Kubernetes operators. Use Cases \u00b6 The OS built by Elemental is intended to be run as the operating system beneath a Rancher Multi-Cluster Management server or as a node in a Kubernetes cluster managed by Rancher. As such it also allows you to build stand alone Kubernetes clusters that run an embedded and smaller version of Rancher to manage the local cluster. A key attribute of Elemental is that it is managed by Rancher and thus Rancher will exist either locally in the cluster or centrally with Rancher Multi-Cluster Manager. OCI Image based \u00b6 Elemental is an image based distribution with an A/B style update mechanism. One first runs on a read-only image A and to do an upgrade pulls a new read only image B and then reboots the system to run on B. What is unique about Elemental is that the runtime images come from OCI Images. Not an OCI Image containing special artifacts, but an actual Docker runnable image that is built using standard Docker build processes. Elemental is built using normal docker build and if you wish to customize the OS image all you need to do is create a new Dockerfile . rancherd \u00b6 Elemental includes no container runtime, Kubernetes distribution, or Rancher itself. All of these assests are dynamically pulled at runtime. All that is included in Elemental is rancher system agent which is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update to containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot. cloud-init \u00b6 Elemental is initially configured using a simple version of cloud-init . It is not expected that one will need to do a lot of customization to Elemental as the core OS's sole purpose is to run Rancher and Kubernetes and not serve as a generic Linux distribution. Elemental Operator \u00b6 Elemental includes an operator that is responsible for managing OS upgrades and managing a secure device inventory to assist with zero touch provisioning. See the full operator docs at Elemental-operator Elemental Teal \u00b6 Elemental Teal is based off of SUSE Linux Enterprise (SLE) Micro for Rancher. There is no specific dependency on SLE beyond that Elemental assumes the underlying distribution is based on systemd. We choose SLE Micro for Rancher for obvious reasons, but beyond that Elemental Teal provides a stable layer to build upon that is well tested and has paths to commercial support, if one chooses.","title":"Architecture"},{"location":"architecture/#architecture","text":"Elemental is a toolkit to build an immutable Linux distribution. It's primary purpose is to run Rancher and its corresponding Kubernetes distributions RKE2 and k3s . But it can be configured for any other workload. That said, the following documentation focusses on a Rancher use-case. Initial node configurations is done using a cloud-init style approach and all further maintenance is done using Kubernetes operators.","title":"Architecture"},{"location":"architecture/#use-cases","text":"The OS built by Elemental is intended to be run as the operating system beneath a Rancher Multi-Cluster Management server or as a node in a Kubernetes cluster managed by Rancher. As such it also allows you to build stand alone Kubernetes clusters that run an embedded and smaller version of Rancher to manage the local cluster. A key attribute of Elemental is that it is managed by Rancher and thus Rancher will exist either locally in the cluster or centrally with Rancher Multi-Cluster Manager.","title":"Use Cases"},{"location":"architecture/#oci-image-based","text":"Elemental is an image based distribution with an A/B style update mechanism. One first runs on a read-only image A and to do an upgrade pulls a new read only image B and then reboots the system to run on B. What is unique about Elemental is that the runtime images come from OCI Images. Not an OCI Image containing special artifacts, but an actual Docker runnable image that is built using standard Docker build processes. Elemental is built using normal docker build and if you wish to customize the OS image all you need to do is create a new Dockerfile .","title":"OCI Image based"},{"location":"architecture/#rancherd","text":"Elemental includes no container runtime, Kubernetes distribution, or Rancher itself. All of these assests are dynamically pulled at runtime. All that is included in Elemental is rancher system agent which is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update to containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot.","title":"rancherd"},{"location":"architecture/#cloud-init","text":"Elemental is initially configured using a simple version of cloud-init . It is not expected that one will need to do a lot of customization to Elemental as the core OS's sole purpose is to run Rancher and Kubernetes and not serve as a generic Linux distribution.","title":"cloud-init"},{"location":"architecture/#elemental-operator","text":"Elemental includes an operator that is responsible for managing OS upgrades and managing a secure device inventory to assist with zero touch provisioning. See the full operator docs at Elemental-operator","title":"Elemental Operator"},{"location":"architecture/#elemental-teal","text":"Elemental Teal is based off of SUSE Linux Enterprise (SLE) Micro for Rancher. There is no specific dependency on SLE beyond that Elemental assumes the underlying distribution is based on systemd. We choose SLE Micro for Rancher for obvious reasons, but beyond that Elemental Teal provides a stable layer to build upon that is well tested and has paths to commercial support, if one chooses.","title":"Elemental Teal"},{"location":"clusters/","text":"Understanding Clusters \u00b6 RancherOS bootstraps a node with Kubernetes (k3s/rke2) and Rancher such that all future management of Kubernetes and Rancher can be done from Kubernetes. This is done by running Rancherd once per node on boot. Once the system has been fully bootstrapped it will not run again. Rancherd is ran from cloud-init and it's configuration is embedded in the cloud-config file. Cluster Initialization \u00b6 Creating a cluster always starts with one node initializing the cluster, and all other nodes joining the cluster by pointing to a server node. The node that will initialize a new cluster is the one with role: server and server: \"\" (empty). The new cluster will have a token generated or you can manually assign a unique string. The token for an existing cluster can be determined by running rancherd get-token on a server node. Joining Nodes \u00b6 Nodes can be joined to the cluster as the role server to add more control plane nodes or as the role agent to add more worker nodes. To join a node you must have the Rancher server URL (which is by default running on port 8443 ) and the token. The server and token are assigned to the server and token fields respectively. Node Roles \u00b6 Rancherd will bootstrap a node with one of the following roles server : Joins the cluster as a new control-plane,etcd,worker node agent : Joins the cluster as a worker only node. Server discovery \u00b6 It can be quite cumbersome to automate bringing up a clustered system that requires one bootstrap node. Also there are more considerations around load balancing and replacing nodes in a proper production setup. Rancherd support server discovery based on go-discover . To use server discovery you must set the role , discovery and token fields. The discovery configuration will be used to dynamically determine what is the server URL and if the current node should act as the node to initialize the cluster. Example role : server discovery : params : # Corresponds to go-discover provider name provider : \"mdns\" # All other key/values are parameters corresponding to what # the go-discover provider is expecting service : \"rancher-server\" # If this is a new cluster it will wait until 3 server are # available and they all agree on the same cluster-init node expectedServers : 3 # How long servers are remembered for. It is useful for providers # that are not consistent in their responses, like mdns. serverCacheDuration : 1m","title":"Understanding Clusters"},{"location":"clusters/#understanding-clusters","text":"RancherOS bootstraps a node with Kubernetes (k3s/rke2) and Rancher such that all future management of Kubernetes and Rancher can be done from Kubernetes. This is done by running Rancherd once per node on boot. Once the system has been fully bootstrapped it will not run again. Rancherd is ran from cloud-init and it's configuration is embedded in the cloud-config file.","title":"Understanding Clusters"},{"location":"clusters/#cluster-initialization","text":"Creating a cluster always starts with one node initializing the cluster, and all other nodes joining the cluster by pointing to a server node. The node that will initialize a new cluster is the one with role: server and server: \"\" (empty). The new cluster will have a token generated or you can manually assign a unique string. The token for an existing cluster can be determined by running rancherd get-token on a server node.","title":"Cluster Initialization"},{"location":"clusters/#joining-nodes","text":"Nodes can be joined to the cluster as the role server to add more control plane nodes or as the role agent to add more worker nodes. To join a node you must have the Rancher server URL (which is by default running on port 8443 ) and the token. The server and token are assigned to the server and token fields respectively.","title":"Joining Nodes"},{"location":"clusters/#node-roles","text":"Rancherd will bootstrap a node with one of the following roles server : Joins the cluster as a new control-plane,etcd,worker node agent : Joins the cluster as a worker only node.","title":"Node Roles"},{"location":"clusters/#server-discovery","text":"It can be quite cumbersome to automate bringing up a clustered system that requires one bootstrap node. Also there are more considerations around load balancing and replacing nodes in a proper production setup. Rancherd support server discovery based on go-discover . To use server discovery you must set the role , discovery and token fields. The discovery configuration will be used to dynamically determine what is the server URL and if the current node should act as the node to initialize the cluster. Example role : server discovery : params : # Corresponds to go-discover provider name provider : \"mdns\" # All other key/values are parameters corresponding to what # the go-discover provider is expecting service : \"rancher-server\" # If this is a new cluster it will wait until 3 server are # available and they all agree on the same cluster-init node expectedServers : 3 # How long servers are remembered for. It is useful for providers # that are not consistent in their responses, like mdns. serverCacheDuration : 1m","title":"Server discovery"},{"location":"configuration/","text":"Configuration Reference \u00b6 All configuration should come from RancherOS minimal cloud-init . Below is a reference of supported configuration. It is important that the config always starts with #cloud-config #cloud-config # Add additional users or set the password/ssh keys for root users : - name : \"bar\" passwd : \"foo\" groups : \"users\" homedir : \"/home/foo\" shell : \"/bin/bash\" ssh_authorized_keys : - faaapploo # Assigns these keys to the first user in users or root if there # is none ssh_authorized_keys : - asdd # Run these commands once the system has fully booted runcmd : - foo # Hostname to assign hostname : \"bar\" # Write arbitrary files write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4 path : /foo/bar permissions : \"0644\" owner : \"bar\" # Rancherd configuration rancherd : ######################################################## # The below parameters apply to server role that first # # initializes the cluster # ######################################################## # The Kubernetes version to be installed. This must be a k3s or RKE2 version # v1.21 or newer. k3s and RKE2 versions always have a `k3s` or `rke2` in the # version string. # Valid versions are # k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' # RKE2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' kubernetesVersion : v1.22.2+k3s1 # The Rancher version to be installed or a channel \"latest\" or \"stable\" rancherVersion : v2.6.0 # Values set on the Rancher Helm chart. Refer to # https://github.com/rancher/rancher/blob/release/v2.6/chart/values.yaml # for possible values. rancherValues : # Below are the default values set # Multi-Cluster Management is disabled by default, change to multi-cluster-management=true to enable features : multi-cluster-management=false # The Rancher UI will run on the host port 8443 by default. Set to 0 to disable # and instead use ingress.enabled=true to route traffic through ingress hostPort : 8443 # Accessing ingress is disabled by default. ingress : enabled : false # Don't create a default admin password noDefaultAdmin : true # The negative value means it will up to that many replicas if there are # at least that many nodes available. For example, if you have 2 nodes and # `replicas` is `-3` then 2 replicas will run. Once you add a third node # a then 3 replicas will run replicas : -3 # External TLS is assumed tls : external # Addition SANs (hostnames) to be added to the generated TLS certificate that # served on port 6443. tlsSans : - additionalhostname.example.com # Kubernetes resources that will be created once Rancher is bootstrapped resources : - kind : ConfigMap apiVersion : v1 metadata : name : random data : key : value # Contents of the registries.yaml that will be used by k3s/RKE2. The structure # is documented at https://rancher.com/docs/k3s/latest/en/installation/private-registry/ registries : {} # The default registry used for all Rancher container images. For more information # refer to https://rancher.com/docs/rancher/v2.6/en/admin-settings/config-private-registry/ systemDefaultRegistry : someprefix.example.com:5000 # Advanced: The system agent installer image used for Kubernetes runtimeInstallerImage : ... # Advanced: The system agent installer image used for Rancher rancherInstallerImage : ... # Generic commands to run before bootstrapping the node. preInstructions : - name : something # This image will be extracted to a temporary folder and # set as the current working dir. The command will not run # contained or chrooted, this is only a way to copy assets # to the host. This is parameter is optional image : custom/image:1.1.1 # Environment variables to set env : - FOO=BAR # Program arguments args : - arg1 - arg2 # Command to run command : /bin/dosomething # Save output to /var/lib/rancher/rancherd/plan/plan-output.json saveOutput : false # Generic commands to run after bootstrapping the node. postInstructions : - name : something env : - FOO=BAR args : - arg1 - arg2 command : /bin/dosomething saveOutput : false ########################################### # The below parameters apply to all roles # ########################################### # The URL to Rancher to join a node. If you have disabled the hostPort and configured # TLS then this will be the server you have setup. server : https://myserver.example.com:8443 # A shared secret to join nodes to the cluster token : sometoken # Instead of setting the server parameter above the server value can be dynamically # determined from cloud provider metadata. This is powered by https://github.com/hashicorp/go-discover. # Discovery requires that the hostPort is not disabled. discovery : params : # Corresponds to go-discover provider name provider : \"mdns\" # All other key/values are parameters corresponding to what # the go-discover provider is expecting service : \"rancher-server\" # If this is a new cluster it will wait until 3 server are # available and they all agree on the same cluster-init node expectedServers : 3 # How long servers are remembered for. It is useful for providers # that are not consistent in their responses, like mdns. serverCacheDuration : 1m # The role of this node. Every cluster must start with one node as role=cluster-init. # After that nodes can be joined using the server role for control-plane nodes and # agent role for worker only nodes. The server/agent terms correspond to the server/agent # terms in k3s and RKE2 role : cluster-init,server,agent # The Kubernetes node name that will be set nodeName : custom-hostname # The IP address that will be set in Kubernetes for this node address : 123.123.123.123 # The internal IP address that will be used for this node internalAddress : 123.123.123.124 # Taints to apply to this node upon creation taints : - dedicated=special-user:NoSchedule # Labels to apply to this node upon creation labels : - key=value","title":"Configuration Reference"},{"location":"configuration/#configuration-reference","text":"All configuration should come from RancherOS minimal cloud-init . Below is a reference of supported configuration. It is important that the config always starts with #cloud-config #cloud-config # Add additional users or set the password/ssh keys for root users : - name : \"bar\" passwd : \"foo\" groups : \"users\" homedir : \"/home/foo\" shell : \"/bin/bash\" ssh_authorized_keys : - faaapploo # Assigns these keys to the first user in users or root if there # is none ssh_authorized_keys : - asdd # Run these commands once the system has fully booted runcmd : - foo # Hostname to assign hostname : \"bar\" # Write arbitrary files write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4 path : /foo/bar permissions : \"0644\" owner : \"bar\" # Rancherd configuration rancherd : ######################################################## # The below parameters apply to server role that first # # initializes the cluster # ######################################################## # The Kubernetes version to be installed. This must be a k3s or RKE2 version # v1.21 or newer. k3s and RKE2 versions always have a `k3s` or `rke2` in the # version string. # Valid versions are # k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' # RKE2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' kubernetesVersion : v1.22.2+k3s1 # The Rancher version to be installed or a channel \"latest\" or \"stable\" rancherVersion : v2.6.0 # Values set on the Rancher Helm chart. Refer to # https://github.com/rancher/rancher/blob/release/v2.6/chart/values.yaml # for possible values. rancherValues : # Below are the default values set # Multi-Cluster Management is disabled by default, change to multi-cluster-management=true to enable features : multi-cluster-management=false # The Rancher UI will run on the host port 8443 by default. Set to 0 to disable # and instead use ingress.enabled=true to route traffic through ingress hostPort : 8443 # Accessing ingress is disabled by default. ingress : enabled : false # Don't create a default admin password noDefaultAdmin : true # The negative value means it will up to that many replicas if there are # at least that many nodes available. For example, if you have 2 nodes and # `replicas` is `-3` then 2 replicas will run. Once you add a third node # a then 3 replicas will run replicas : -3 # External TLS is assumed tls : external # Addition SANs (hostnames) to be added to the generated TLS certificate that # served on port 6443. tlsSans : - additionalhostname.example.com # Kubernetes resources that will be created once Rancher is bootstrapped resources : - kind : ConfigMap apiVersion : v1 metadata : name : random data : key : value # Contents of the registries.yaml that will be used by k3s/RKE2. The structure # is documented at https://rancher.com/docs/k3s/latest/en/installation/private-registry/ registries : {} # The default registry used for all Rancher container images. For more information # refer to https://rancher.com/docs/rancher/v2.6/en/admin-settings/config-private-registry/ systemDefaultRegistry : someprefix.example.com:5000 # Advanced: The system agent installer image used for Kubernetes runtimeInstallerImage : ... # Advanced: The system agent installer image used for Rancher rancherInstallerImage : ... # Generic commands to run before bootstrapping the node. preInstructions : - name : something # This image will be extracted to a temporary folder and # set as the current working dir. The command will not run # contained or chrooted, this is only a way to copy assets # to the host. This is parameter is optional image : custom/image:1.1.1 # Environment variables to set env : - FOO=BAR # Program arguments args : - arg1 - arg2 # Command to run command : /bin/dosomething # Save output to /var/lib/rancher/rancherd/plan/plan-output.json saveOutput : false # Generic commands to run after bootstrapping the node. postInstructions : - name : something env : - FOO=BAR args : - arg1 - arg2 command : /bin/dosomething saveOutput : false ########################################### # The below parameters apply to all roles # ########################################### # The URL to Rancher to join a node. If you have disabled the hostPort and configured # TLS then this will be the server you have setup. server : https://myserver.example.com:8443 # A shared secret to join nodes to the cluster token : sometoken # Instead of setting the server parameter above the server value can be dynamically # determined from cloud provider metadata. This is powered by https://github.com/hashicorp/go-discover. # Discovery requires that the hostPort is not disabled. discovery : params : # Corresponds to go-discover provider name provider : \"mdns\" # All other key/values are parameters corresponding to what # the go-discover provider is expecting service : \"rancher-server\" # If this is a new cluster it will wait until 3 server are # available and they all agree on the same cluster-init node expectedServers : 3 # How long servers are remembered for. It is useful for providers # that are not consistent in their responses, like mdns. serverCacheDuration : 1m # The role of this node. Every cluster must start with one node as role=cluster-init. # After that nodes can be joined using the server role for control-plane nodes and # agent role for worker only nodes. The server/agent terms correspond to the server/agent # terms in k3s and RKE2 role : cluster-init,server,agent # The Kubernetes node name that will be set nodeName : custom-hostname # The IP address that will be set in Kubernetes for this node address : 123.123.123.123 # The internal IP address that will be used for this node internalAddress : 123.123.123.124 # Taints to apply to this node upon creation taints : - dedicated=special-user:NoSchedule # Labels to apply to this node upon creation labels : - key=value","title":"Configuration Reference"},{"location":"customizing/","text":"Custom Images \u00b6 Elemental image can easily be remastered using a docker build. For example, to add cowsay to Elemental you would use the following Dockerfile Docker image \u00b6 # The version of Elemental to modify FROM rancher-sandbox/os2:VERSION # Your custom commands RUN zypper install -y cowsay # IMPORTANT: /usr/lib/Elemental-release is used for versioning/upgrade. The # values here should reflect the tag of the image currently being built ARG IMAGE_REPO = norepo ARG IMAGE_TAG = latest RUN echo \"IMAGE_REPO= ${ IMAGE_REPO } \" > /usr/lib/rancheros-release && \\ echo \"IMAGE_TAG= ${ IMAGE_TAG } \" >> /usr/lib/rancheros-release && \\ echo \"IMAGE= ${ IMAGE_REPO } : ${ IMAGE_TAG } \" >> /usr/lib/rancheros-release Where VERSION is the base version we want to customize. All version numbers available at quay.io or github And then the following commands docker build --build-arg IMAGE_REPO = myrepo/custom-build \\ --build-arg IMAGE_TAG = v1.1.1 \\ -t myrepo/custom-build:v1.1.1 . docker push myrepo/custom-build:v1.1.1 Your new customized OS is available at in the docker image myrepo/custom-build:v1.1.1 and you can check out your new image using docker with docker run -it myrepo/custom-build:v1.1.1 bash Bootable images \u00b6 To create bootable images from the docker image you just created run the below command # Download the ros-image-build script curl -o ros-image-build https://raw.githubusercontent.com/rancher/elemental/main/ros-image-build # Run the script creating a qcow image, an ISO, and an AMI bash ros-image-build myrepo/custom-build:v1.1.1 qcow,iso,ami The above command will create an ISO, a qcow image, and publish AMIs. You need not create all three types and can change to comma seperated list to the types you care for. Auto-installing ISO \u00b6 To create an ISO that upon boot will automatically run an installation, as an alternative to iPXE install, run the following command. bash ros-image-build myrepo/custom-build:v1.1.1 iso mycloud-config-file.txt The third parameter is a path to a file that will be used as the cloud config passed to the installation. Refer to the installation and configuration reference for the contents of the file.","title":"Custom Images"},{"location":"customizing/#custom-images","text":"Elemental image can easily be remastered using a docker build. For example, to add cowsay to Elemental you would use the following Dockerfile","title":"Custom Images"},{"location":"customizing/#docker-image","text":"# The version of Elemental to modify FROM rancher-sandbox/os2:VERSION # Your custom commands RUN zypper install -y cowsay # IMPORTANT: /usr/lib/Elemental-release is used for versioning/upgrade. The # values here should reflect the tag of the image currently being built ARG IMAGE_REPO = norepo ARG IMAGE_TAG = latest RUN echo \"IMAGE_REPO= ${ IMAGE_REPO } \" > /usr/lib/rancheros-release && \\ echo \"IMAGE_TAG= ${ IMAGE_TAG } \" >> /usr/lib/rancheros-release && \\ echo \"IMAGE= ${ IMAGE_REPO } : ${ IMAGE_TAG } \" >> /usr/lib/rancheros-release Where VERSION is the base version we want to customize. All version numbers available at quay.io or github And then the following commands docker build --build-arg IMAGE_REPO = myrepo/custom-build \\ --build-arg IMAGE_TAG = v1.1.1 \\ -t myrepo/custom-build:v1.1.1 . docker push myrepo/custom-build:v1.1.1 Your new customized OS is available at in the docker image myrepo/custom-build:v1.1.1 and you can check out your new image using docker with docker run -it myrepo/custom-build:v1.1.1 bash","title":"Docker image"},{"location":"customizing/#bootable-images","text":"To create bootable images from the docker image you just created run the below command # Download the ros-image-build script curl -o ros-image-build https://raw.githubusercontent.com/rancher/elemental/main/ros-image-build # Run the script creating a qcow image, an ISO, and an AMI bash ros-image-build myrepo/custom-build:v1.1.1 qcow,iso,ami The above command will create an ISO, a qcow image, and publish AMIs. You need not create all three types and can change to comma seperated list to the types you care for.","title":"Bootable images"},{"location":"customizing/#auto-installing-iso","text":"To create an ISO that upon boot will automatically run an installation, as an alternative to iPXE install, run the following command. bash ros-image-build myrepo/custom-build:v1.1.1 iso mycloud-config-file.txt The third parameter is a path to a file that will be used as the cloud config passed to the installation. Refer to the installation and configuration reference for the contents of the file.","title":"Auto-installing ISO"},{"location":"dashboard/","text":"Dashboard/UI \u00b6 The Rancher UI is running by default on port :8443 . There default admin user password set is a long random string. You can run rancherd reset-admin to get a new admin password to login. To disable the Rancher UI from running on a host port, or to change the default hostPort used the below configuration. #cloud-config rancherd : rancherValues : # Setting the host port to 0 will disable the hostPort, default is 8443 hostPort : 0","title":"Dashboard/UI"},{"location":"dashboard/#dashboardui","text":"The Rancher UI is running by default on port :8443 . There default admin user password set is a long random string. You can run rancherd reset-admin to get a new admin password to login. To disable the Rancher UI from running on a host port, or to change the default hostPort used the below configuration. #cloud-config rancherd : rancherValues : # Setting the host port to 0 will disable the hostPort, default is 8443 hostPort : 0","title":"Dashboard/UI"},{"location":"installation/","text":"Installation \u00b6 Overview \u00b6 The design of Elemental is that you boot from a vanilla image and through cloud-init and Kubernetes mechanisms the node will be configured. Installation of Elemental is really the process of building an image from which you can boot. During the image building process you can bake in default OEM configuration that is a part of the image. Installation Configuration \u00b6 The installation process is driven by a single config file. The configuration file contains the installation directives and the OEM configuration for the image. The installation configuration should be hosted on an HTTP or TFTP server. A simple approach is to use a GitHub Gist . Kernel Command Line \u00b6 Install directives can be set from the kernel command line using a period (.) seperated key structure such as Elemental.install.config_url . They kernel command line keys are case-insensitive. Reference \u00b6 #cloud-config Elemental : install : # An http://, https://, or tftp:// URL to load as the base configuration # for this configuration. This configuration can include any install # directives or OEM configuration. The resulting merged configuration # will be read by the installer and all content of the merged config will # be stored in /oem/99_custom.yaml in the created image. configURL : http://example.com/machine-cloud-config # Turn on verbose logging for the installation process debug : false # The target device that will be formatted and grub will be install on. # The partition table will be cleared and recreated with the default # partition layout. If noFormat is set to true this parameter is only # used to install grub. device : /dev/vda # If the system has the path /sys/firmware/efi it will be treated as a # UEFI system. If you are creating an UEFI image on a non-EFI platform # then this flag will force the installer to use UEFI even if not detected. forceEFI : false # If true then it is assumed that the disk is already formatted with the standard # partitions need by Elemental. Refer to the partition table section below for the # exact requirements. Also, if this is set to true noFormat : false # After installation the system will reboot by default. If you wish to instead # power off the system set this to true. powerOff : false # The installed image will set the default console to the current TTY value # used during the installation. To force the installation to use a different TTY # then set that value here. tty : ttyS0 # Any other cloud-init values can be included in this file and will be stored in # /oem/99_custom.yaml of the installed image ISO Installation \u00b6 When booting from the ISO you will immediately be presented with the shell. The root password is hard coded to ros if needed. A SSH server will be running so realize that because of the hard coded password this is an insecure system to be running on a public network. From the shell run the below where ${LOCATION} should be a path to a local file or http:// , https:// , or tftp:// URL. ros-installer -config-file ${ LOCATION } Interactive \u00b6 ros-installer can also be run without any arguments to allow you to install a simple vanilla image with a root password set. iPXE Installation \u00b6 Download the latest ipxe script from current release Partition Table \u00b6 Elemental requires the following partitions. These partitions are required by cOS-toolkit Label Default Size Contains COS_BOOT 50 MiB UEFI Boot partition COS_STATE 15 GiB A/B bootable file system images constructed from OCI images COS_OEM 50 MiB OEM cloud-config files and other data COS_RECOVERY 8 GiB Recovery file system image if COS_STATE is destroyed COS_PERSISTENT Remaining space All contents of the persistent folders Folders \u00b6 Path Read-Only Ephemeral Persistent / x /etc x /etc/cni x /etc/iscsi x /etc/rancher x /etc/ssh x /etc/systemd x /srv x /home x /opt x /root x /var x /usr/libexec x /var/lib/cni x /var/lib/kubelet x /var/lib/longhorn x /var/lib/rancher x /var/lib/wicked x /var/log x","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#overview","text":"The design of Elemental is that you boot from a vanilla image and through cloud-init and Kubernetes mechanisms the node will be configured. Installation of Elemental is really the process of building an image from which you can boot. During the image building process you can bake in default OEM configuration that is a part of the image.","title":"Overview"},{"location":"installation/#installation-configuration","text":"The installation process is driven by a single config file. The configuration file contains the installation directives and the OEM configuration for the image. The installation configuration should be hosted on an HTTP or TFTP server. A simple approach is to use a GitHub Gist .","title":"Installation Configuration"},{"location":"installation/#kernel-command-line","text":"Install directives can be set from the kernel command line using a period (.) seperated key structure such as Elemental.install.config_url . They kernel command line keys are case-insensitive.","title":"Kernel Command Line"},{"location":"installation/#reference","text":"#cloud-config Elemental : install : # An http://, https://, or tftp:// URL to load as the base configuration # for this configuration. This configuration can include any install # directives or OEM configuration. The resulting merged configuration # will be read by the installer and all content of the merged config will # be stored in /oem/99_custom.yaml in the created image. configURL : http://example.com/machine-cloud-config # Turn on verbose logging for the installation process debug : false # The target device that will be formatted and grub will be install on. # The partition table will be cleared and recreated with the default # partition layout. If noFormat is set to true this parameter is only # used to install grub. device : /dev/vda # If the system has the path /sys/firmware/efi it will be treated as a # UEFI system. If you are creating an UEFI image on a non-EFI platform # then this flag will force the installer to use UEFI even if not detected. forceEFI : false # If true then it is assumed that the disk is already formatted with the standard # partitions need by Elemental. Refer to the partition table section below for the # exact requirements. Also, if this is set to true noFormat : false # After installation the system will reboot by default. If you wish to instead # power off the system set this to true. powerOff : false # The installed image will set the default console to the current TTY value # used during the installation. To force the installation to use a different TTY # then set that value here. tty : ttyS0 # Any other cloud-init values can be included in this file and will be stored in # /oem/99_custom.yaml of the installed image","title":"Reference"},{"location":"installation/#iso-installation","text":"When booting from the ISO you will immediately be presented with the shell. The root password is hard coded to ros if needed. A SSH server will be running so realize that because of the hard coded password this is an insecure system to be running on a public network. From the shell run the below where ${LOCATION} should be a path to a local file or http:// , https:// , or tftp:// URL. ros-installer -config-file ${ LOCATION }","title":"ISO Installation"},{"location":"installation/#interactive","text":"ros-installer can also be run without any arguments to allow you to install a simple vanilla image with a root password set.","title":"Interactive"},{"location":"installation/#ipxe-installation","text":"Download the latest ipxe script from current release","title":"iPXE Installation"},{"location":"installation/#partition-table","text":"Elemental requires the following partitions. These partitions are required by cOS-toolkit Label Default Size Contains COS_BOOT 50 MiB UEFI Boot partition COS_STATE 15 GiB A/B bootable file system images constructed from OCI images COS_OEM 50 MiB OEM cloud-config files and other data COS_RECOVERY 8 GiB Recovery file system image if COS_STATE is destroyed COS_PERSISTENT Remaining space All contents of the persistent folders","title":"Partition Table"},{"location":"installation/#folders","text":"Path Read-Only Ephemeral Persistent / x /etc x /etc/cni x /etc/iscsi x /etc/rancher x /etc/ssh x /etc/systemd x /srv x /home x /opt x /root x /var x /usr/libexec x /var/lib/cni x /var/lib/kubelet x /var/lib/longhorn x /var/lib/rancher x /var/lib/wicked x /var/log x","title":"Folders"},{"location":"mcm/","text":"Multi-Cluster Management \u00b6 By default Multi Cluster Managmement is disabled in Rancher. To enable set the following in the rancherd config.yaml #cloud-config rancherd : rancherValues : features : - multi-cluster-management=true","title":"Multi-Cluster Management"},{"location":"mcm/#multi-cluster-management","text":"By default Multi Cluster Managmement is disabled in Rancher. To enable set the following in the rancherd config.yaml #cloud-config rancherd : rancherValues : features : - multi-cluster-management=true","title":"Multi-Cluster Management"},{"location":"upgrade/","text":"Upgrade \u00b6 Command line \u00b6 You can also use the rancherd upgrade command on a server node to automatically upgrade Elemental, Rancher, and/or Kubernetes. Kubernetes API \u00b6 All components in Elemental are managed using Kubernetes. Below is how to use Kubernetes approaches to upgrade the components. Elemental \u00b6 Elemental is upgraded with the Elemental operator. Refer to the Elemental Operator documentation for complete information, but the TL;DR is kubectl edit -n fleet-local default-os-image apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : default-os-image namespace : fleet-local spec : # Set to the new Elemental version you would like to upgrade to osImage : quay.io/costoolkit/os2:v0.0.0 Managing available versions \u00b6 An upgrade channel file ( rancheros-v0.0.0-amd64.upgradechannel-amd64.yaml ) file is shipped in Elemental releases and can be applied in a Kubernetes cluster where the rancheros operator is installed to syncronize available version for upgrades. For instance an upgrade channel file might look like this and is sufficient to kubectl apply it where the ros-operator is installed: apiVersion : rancheros.cattle.io/v1 kind : ManagedOSVersionChannel metadata : name : os2-amd64 namespace : fleet-default spec : options : args : - github command : - /usr/bin/upgradechannel-discovery envs : - name : REPOSITORY value : rancher-sandbox/os2 - name : IMAGE_PREFIX value : quay.io/costoolkit/os2-ci - name : VERSION_SUFFIX value : -amd64 image : quay.io/costoolkit/upgradechannel-discovery:v0.3-4b83dbe type : custom Note: the namespace here is set by default to fleet-default , that can be changed to fleet-local to target instead the local clusters. The operator will syncronize available versions and populate ManagedOSVersion accordingly. To trigger an upgrade from a ManagedOSVersion refer to its name in the ManagedOSImage field, instead of an osImage : kubectl edit -n fleet-local default-os-image apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : default-os-image namespace : fleet-local spec : # Set to the new ManagedOSVersion you would like to upgrade to managedOSVersionName : v0.1.0-alpha22-amd64 Note: be sure to have osImage empty when refering to a ManagedOSVersion as it takes precedence over ManagedOSVersion s. system-agent \u00b6 Rancher system agent itself doesn't need to be upgraded. It is only ran once per node to bootstrap the system and then after that provides no value. Rancher system agent is packaged in the OS image so newer versions of Rancher system agent will come with newer versions of Elemental. Rancher \u00b6 Rancher is installed as a helm chart following the standard procedure. You can upgrade Rancher with the standard procedure documented . Kubernetes \u00b6 To upgrade Kubernetes you will use Rancher to orchestrate the upgrade. This is a matter of changing the Kubernetes version on the fleet-local/local Cluster in the provisioning.cattle.io/v1 apiVersion. For example kubectl edit clusters.provisioning.cattle.io -n fleet-local local apiVersion : provisioning.cattle.io/v1 kind : Cluster metadata : name : local namespace : fleet-local spec : # Change to new valid k8s version, >= 1.21 # Valid versions are # k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' # RKE2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' kubernetesVersion : v1.21.4+k3s1","title":"Upgrade"},{"location":"upgrade/#upgrade","text":"","title":"Upgrade"},{"location":"upgrade/#command-line","text":"You can also use the rancherd upgrade command on a server node to automatically upgrade Elemental, Rancher, and/or Kubernetes.","title":"Command line"},{"location":"upgrade/#kubernetes-api","text":"All components in Elemental are managed using Kubernetes. Below is how to use Kubernetes approaches to upgrade the components.","title":"Kubernetes API"},{"location":"upgrade/#elemental","text":"Elemental is upgraded with the Elemental operator. Refer to the Elemental Operator documentation for complete information, but the TL;DR is kubectl edit -n fleet-local default-os-image apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : default-os-image namespace : fleet-local spec : # Set to the new Elemental version you would like to upgrade to osImage : quay.io/costoolkit/os2:v0.0.0","title":"Elemental"},{"location":"upgrade/#managing-available-versions","text":"An upgrade channel file ( rancheros-v0.0.0-amd64.upgradechannel-amd64.yaml ) file is shipped in Elemental releases and can be applied in a Kubernetes cluster where the rancheros operator is installed to syncronize available version for upgrades. For instance an upgrade channel file might look like this and is sufficient to kubectl apply it where the ros-operator is installed: apiVersion : rancheros.cattle.io/v1 kind : ManagedOSVersionChannel metadata : name : os2-amd64 namespace : fleet-default spec : options : args : - github command : - /usr/bin/upgradechannel-discovery envs : - name : REPOSITORY value : rancher-sandbox/os2 - name : IMAGE_PREFIX value : quay.io/costoolkit/os2-ci - name : VERSION_SUFFIX value : -amd64 image : quay.io/costoolkit/upgradechannel-discovery:v0.3-4b83dbe type : custom Note: the namespace here is set by default to fleet-default , that can be changed to fleet-local to target instead the local clusters. The operator will syncronize available versions and populate ManagedOSVersion accordingly. To trigger an upgrade from a ManagedOSVersion refer to its name in the ManagedOSImage field, instead of an osImage : kubectl edit -n fleet-local default-os-image apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : default-os-image namespace : fleet-local spec : # Set to the new ManagedOSVersion you would like to upgrade to managedOSVersionName : v0.1.0-alpha22-amd64 Note: be sure to have osImage empty when refering to a ManagedOSVersion as it takes precedence over ManagedOSVersion s.","title":"Managing available versions"},{"location":"upgrade/#system-agent","text":"Rancher system agent itself doesn't need to be upgraded. It is only ran once per node to bootstrap the system and then after that provides no value. Rancher system agent is packaged in the OS image so newer versions of Rancher system agent will come with newer versions of Elemental.","title":"system-agent"},{"location":"upgrade/#rancher","text":"Rancher is installed as a helm chart following the standard procedure. You can upgrade Rancher with the standard procedure documented .","title":"Rancher"},{"location":"upgrade/#kubernetes","text":"To upgrade Kubernetes you will use Rancher to orchestrate the upgrade. This is a matter of changing the Kubernetes version on the fleet-local/local Cluster in the provisioning.cattle.io/v1 apiVersion. For example kubectl edit clusters.provisioning.cattle.io -n fleet-local local apiVersion : provisioning.cattle.io/v1 kind : Cluster metadata : name : local namespace : fleet-local spec : # Change to new valid k8s version, >= 1.21 # Valid versions are # k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' # RKE2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' kubernetesVersion : v1.21.4+k3s1","title":"Kubernetes"},{"location":"versions/","text":"Supported Versions and Channels \u00b6 The kubernetesVersion and rancherVersion fields accept explicit versions numbers or channel names. Valid Versions \u00b6 The list of valid versions for the kubernetesVersion field can be determined from the Rancher metadata using the following commands. k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' rke2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' The list of valid rancherVersion values can be obtained from the stable and latest helm repos. The version string is expected to be the \"application version\" which is the version starting with a v . For example, v2.6.2 is the current format not 2.6.2 . Version Channels \u00b6 Valid kubernetesVersion channels are as follows: Channel Name Description stable k3s stable (default value of kubernetesVersion) latest k3s latest testing k3s test stable:k3s Same as stable channel latest:k3s Same as latest channel testing:k3s Same as testing channel stable:rke2 rke2 stable latest:rke2 rke2 latest testing:rke2 rke2 testing v1.21 Latest k3s v1.21 release. The applies to any Kubernetes minor version v1.21:rke2 Latest rke2 v1.21 release. The applies to any Kubernetes minor version Valid rancherVersions channels are as follows: Channel Name Description stable stable helm repo latest latest helm repo","title":"Supported Versions and Channels"},{"location":"versions/#supported-versions-and-channels","text":"The kubernetesVersion and rancherVersion fields accept explicit versions numbers or channel names.","title":"Supported Versions and Channels"},{"location":"versions/#valid-versions","text":"The list of valid versions for the kubernetesVersion field can be determined from the Rancher metadata using the following commands. k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' rke2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' The list of valid rancherVersion values can be obtained from the stable and latest helm repos. The version string is expected to be the \"application version\" which is the version starting with a v . For example, v2.6.2 is the current format not 2.6.2 .","title":"Valid Versions"},{"location":"versions/#version-channels","text":"Valid kubernetesVersion channels are as follows: Channel Name Description stable k3s stable (default value of kubernetesVersion) latest k3s latest testing k3s test stable:k3s Same as stable channel latest:k3s Same as latest channel testing:k3s Same as testing channel stable:rke2 rke2 stable latest:rke2 rke2 latest testing:rke2 rke2 testing v1.21 Latest k3s v1.21 release. The applies to any Kubernetes minor version v1.21:rke2 Latest rke2 v1.21 release. The applies to any Kubernetes minor version Valid rancherVersions channels are as follows: Channel Name Description stable stable helm repo latest latest helm repo","title":"Version Channels"}]}